{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "762c29ab",
   "metadata": {},
   "source": [
    "# Analyze classifier performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1623494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyprojroot import here\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eb334f",
   "metadata": {},
   "source": [
    "Read annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a8aa615",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = pd.read_excel(here(r'02_Data/01_Webdata/01_Annotation/annotation_annotater1.xlsx'), sheet_name='Annotate_here')\n",
    "a2 = pd.read_excel(here(r'02_Data/01_Webdata/01_Annotation/annotation_annotater2.xlsx'), sheet_name='Annotate_here')\n",
    "# Reduce to first 1000 rows which were annotated only\n",
    "a1 = a1\n",
    "a2 = a2.iloc[:1000]\n",
    "# Merge annotations\n",
    "a = a2.merge(a1, on=['ID', 'Reference'], how='left')\n",
    "a = a[['ID', 'Reference', 'Label_x', 'Label_y']].rename(columns={'Reference': 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfd77977",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'p': 'problem',\n",
    "             'c': 'no_problem',\n",
    "             'a': 'adaption',\n",
    "             'i': 'information',\n",
    "              np.nan: 'unclear'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "954bd38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a['True1'] = a.Label_x.map(label_map)\n",
    "a['True2'] = a.Label_y.map(label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce64fc9",
   "metadata": {},
   "source": [
    "Calculate inter-annotator agreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38015956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.714"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(a.True1==a.True2)/len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a84ea6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    adaption       0.80      0.67      0.73       215\n",
      " information       0.49      0.74      0.59       158\n",
      "  no_problem       0.47      0.75      0.58        24\n",
      "     problem       0.96      0.81      0.88       358\n",
      "     unclear       0.60      0.59      0.59       245\n",
      "\n",
      "    accuracy                           0.71      1000\n",
      "   macro avg       0.67      0.71      0.68      1000\n",
      "weighted avg       0.75      0.71      0.72      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(a.True1, a.True2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4ffbfbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6214668029033232"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(a.True1, a.True2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba59528d",
   "metadata": {},
   "source": [
    "Read classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be249f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.read_csv(here(r'02_Data/01_Webdata/01_Annotation/corona_predictions_sample.csv'), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49df962f",
   "metadata": {},
   "source": [
    "Merge annotation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c35ddf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = a.merge(pred, how='left', on=['ID'])\n",
    "# Reduce to references with agreement between both annotators\n",
    "cl = cl.loc[cl.True1==cl.True2]\n",
    "cl.drop_duplicates(['ID', 'text_x'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59159e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    adaption       0.64      0.94      0.77       144\n",
      " information       0.66      0.95      0.78       117\n",
      "  no_problem       1.00      0.22      0.36        18\n",
      "     problem       0.99      0.98      0.98       290\n",
      "     unclear       0.93      0.30      0.45       145\n",
      "\n",
      "    accuracy                           0.81       714\n",
      "   macro avg       0.85      0.68      0.67       714\n",
      "weighted avg       0.86      0.81      0.78       714\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(cl.True1, cl.vote))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
